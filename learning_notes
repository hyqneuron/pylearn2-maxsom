

TODOs:
  * Fix BLAS problem
  * Add SOM annealing
  * Find out how convolution is done and how to add SOM to conv


Structure of overall program

train.py builds Train class using serial.load(model_file_path)
Train.main_loop()
    self.setup() # setup algorithm.setup(), monitor, and extensions
    run_callbacks_and_monitoring() # call self.model.monitor() and extension.on_monitor
    for each epoch:
        self.algorithm.train()
            for each batch:
                algorithm.sgd_update() # sgd_update created by algorithm.setup()
                    in the default implementation of sgd_update(),
                    cost is computed using algorithm.cost.expr()
                    grad, updates = algorithm.cost.get_gradient(), default is just T.grad(cost,params)
                    updates then updated using algorithm.learning_rule.get_updates OR standard param-LR*grad
                    model.modify_updates()
                        MLP._modify_updates() calls each layer's modify_updates()
                monitor.report_batch & update_callbacks
            parameter infinity checks
        run_callbacks_and_monitoring() and 
        self.algorithm.continue_learning()
    save things and exit

Question: does dropout drop each Maxout group, or drop all units?
Answer: MLP.apply_dropout() applies dropout to the input of each layer. Which
means Maxout groups get dropped, not individual units within each group.
Detailed procedure of dropout:
    Dropout.expr() calls model.dropout_fprop
    MLP.dropout_fprop()
        calls MLP.apply_dropout on the input to each layer first
        Then calls Layer.fprop using the dropped-out input

Question: Maxout has its own fprop, but where is it called?
The default mnist_pi.yaml
    uses Dropout as cost, MLP as model, Maxout as layer
    Maxout defines its own fprop
    Dropout doesn't define its own get_gradients
    Dropout defines its own expr()
        its expr() calls model.dropout_fprop
        model.dropout_fprop defined in MLP
        MLP.dropout_fprop calls layer.fprop

