

TODOs:
  * Fix SOM annealing
  * Add SOM to MaxoutConvC01B
  * Create SOMAX-autoencoder
  * Add Temporal cost to SOMAX-autoencoder

LocateReLU TODOs:
  * Figure out how to pass in a single data sample for classification
  * Figure out how to build a Theano function to compute input-contribution
    - T.multiply (d_label/d_input, input)


Structure of overall program

train.py builds Train class using serial.load(model_file_path)
Train.main_loop()
    self.setup() # setup algorithm.setup(), monitor, and extensions
    run_callbacks_and_monitoring() # call self.model.monitor() and extension.on_monitor
    for each epoch:
        self.algorithm.train()
            for each batch:
                algorithm.sgd_update() # sgd_update created by algorithm.setup()
                    in the default implementation of sgd_update(),
                    cost is computed using algorithm.cost.expr()
                    grad, updates = algorithm.cost.get_gradient(), default is just T.grad(cost,params)
                    model.modify_updates()
                        MLP._modify_updates() calls each layer's modify_updates()
                monitor.report_batch & update_callbacks
            parameter infinity checks
        run_callbacks_and_monitoring() and 
        self.algorithm.continue_learning()
    save things and exit

Question: does dropout drop each Maxout group, or drop all units?
Answer: MLP.apply_dropout() applies dropout to the input of each layer. Which
means Maxout groups get dropped, not individual units within each group.
Detailed procedure of dropout:
    Dropout.expr() calls model.dropout_fprop
    MLP.dropout_fprop()
        calls MLP.apply_dropout on the input to each layer first
        Then calls Layer.fprop using the dropped-out input

Question: Maxout has its own fprop, but where is it called?
The default mnist_pi.yaml
    uses Dropout as cost, MLP as model, Maxout as layer
    Maxout defines its own fprop
    Dropout doesn't define its own get_gradients
    Dropout defines its own expr()
        its expr() calls model.dropout_fprop
        model.dropout_fprop defined in MLP
        MLP.dropout_fprop calls layer.fprop

Question: How does MaxoutConvC01B do convolution and cross-channel max pooling?
Answer: MaxoutConvC01B.set_input_space() sets up the transformer by calling 
setup_detector_layer_c01b, which creates a Conv2D transformer.
Inside MaxoutConvC01B, fprop uses transformer (conv2d_c01b.Conv2D) to compute
z=x*filter, then performs spatial max-pooling and cross-channel max-pooling
using max_pool_c01b and T.maximum respectively

We just have to figure out a way to modify the gradient computation
