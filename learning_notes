

What I intend to do:
  * Find out how convolution is done
  * Find out how max pooling is performed: by Maxout's fprop


Structure of overall program

train.py builds Train class using serial.load(model_file_path)
Train.main_loop()
    self.setup() # setup algorithm.setup(), monitor, and extensions
    self.algorithm.train()
        algorithm.sgd_update() # sgd_update created by algorithm.setup()
            in the default implementation of sgd_update(),
            cost is computed using algorithm.cost.expr()
            grad, updates = algorithm.cost.get_gradient(), default is just T.grad(cost,params)
            updates then updated using
                algorithm.learning_rule.get_updates OR standard param-LR*grad
                model.modify_updates()
                    calls subclass's _modify_updates()
                    MLP._modify_updates() calls each layer's modify_updates()
                    
        check continue_learning using algorithm.continue_learning() and extension_continue
    save things and exit

Question: does dropout drop each Maxout group, or drop all units?
Answer: MLP.apply_dropout() applies dropout to the input of each layer. Which
means Maxout groups get dropped, not individual units within each group.
Detailed procedure of dropout:
    Dropout.expr() calls model.dropout_fprop
    MLP.dropout_fprop()
        calls MLP.apply_dropout on the input to each layer first
        Then calls Layer.fprop using the dropped-out input

Question: Maxout has its own fprop, but where is it called?
The default mnist_pi.yaml
    uses Dropout as cost, MLP as model, Maxout as layer
    Maxout defines its own fprop
    Dropout doesn't define its own get_gradients
    Dropout defines its own expr()
        its expr() calls model.dropout_fprop
        model.dropout_fprop defined in MLP
        MLP.dropout_fprop calls layer.fprop


TODOs:
Now that I have found out how to manipulate the updates, the next steps to
perform:
1. Add copying matrix annealing


